{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnovaYoung/Classifying-Music-Genre-with-CNNs-and-LSTM-Models/blob/main/Music_Genre_and_Composer_Classification_Using_Deep_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcHyRa9mZeGi"
      },
      "source": [
        "**Classifying Music Genre and Composers Using CNNs and LSTM Models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYi2XN1XZeGi"
      },
      "source": [
        "I'm going to load the data and create new directories: This will ensure that only the relevant composers' files are loaded for further processing. I only care to pull out Chopin, Bach, Beethoven, and Mozart."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TbSMgE-3ZeGi",
        "outputId": "85f74513-97d6-4b84-daa3-0b23476adf3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MIDI files have been organized by composer.\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "zip_path = r'\\Users\\manov\\Downloads\\archive (3).zip'\n",
        "extract_dir = r'\\Users\\manov\\Downloads\\extracted_composers'\n",
        "\n",
        "if not os.path.exists(extract_dir):\n",
        "    os.makedirs(extract_dir)\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_dir)\n",
        "\n",
        "# List of composers I'm interested in\n",
        "composers_of_interest = ['Bach', 'Beethoven', 'Chopin', 'Mozart']\n",
        "\n",
        "# Create directories for the selected composers\n",
        "for composer in composers_of_interest:\n",
        "    composer_dir = os.path.join(extract_dir, composer)\n",
        "    if not os.path.exists(composer_dir):\n",
        "        os.makedirs(composer_dir)\n",
        "\n",
        "# Move the MIDI files of the composers of interest to their respective directories\n",
        "for root, dirs, files in os.walk(extract_dir):\n",
        "    for file in files:\n",
        "        if file.endswith('.mid') or file.endswith('.midi'):\n",
        "            file_path = os.path.join(root, file)\n",
        "            # Check if the file belongs to one of the composers of interest\n",
        "            for composer in composers_of_interest:\n",
        "                if composer in root or composer in file:\n",
        "                    composer_dir = os.path.join(extract_dir, composer)\n",
        "                    destination_path = os.path.join(composer_dir, file)\n",
        "                    if os.path.exists(destination_path):\n",
        "                        # File already exists, skip or rename it\n",
        "                        continue\n",
        "                    shutil.move(file_path, composer_dir)\n",
        "                    break\n",
        "\n",
        "print(\"MIDI files have been organized by composer.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpKUZVFBZeGj"
      },
      "source": [
        "**DATA PREPROCESSING**\n",
        "\n",
        " The next step is to preprocess the data. This involves converting the MIDI files into a format suitable for my deep learning models and applying data augmentation techniques such as pitch shifting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IjxpbzTUZeGj",
        "outputId": "d917e236-4718-4f12-908b-658db6400c8c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\manov\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pretty_midi\\pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error processing C:\\Users\\manov\\Downloads\\extracted_composers\\Beethoven\\Anhang 14-3.mid: Could not decode key with 3 flats and mode 255\n",
            "Error processing C:\\Users\\manov\\Downloads\\extracted_composers\\Mozart\\K281 Piano Sonata n03 3mov.mid: Could not decode key with 2 flats and mode 2\n"
          ]
        }
      ],
      "source": [
        "import pretty_midi\n",
        "import pretty_midi.utilities\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Define the path to the directories for each composer\n",
        "base_dir = r'C:\\Users\\manov\\Downloads\\extracted_composers'\n",
        "composers = ['Bach', 'Beethoven', 'Chopin', 'Mozart']\n",
        "\n",
        "def load_midi_files(base_dir, composers):\n",
        "    midi_files = []\n",
        "    labels = []\n",
        "    for composer in composers:\n",
        "        composer_dir = os.path.join(base_dir, composer)\n",
        "        for file_name in os.listdir(composer_dir):\n",
        "            if file_name.endswith('.mid') or file_name.endswith('.midi'):\n",
        "                midi_files.append(os.path.join(composer_dir, file_name))\n",
        "                labels.append(composer)\n",
        "    return midi_files, labels\n",
        "\n",
        "def midi_to_notes(midi_file):\n",
        "    try:\n",
        "        midi_data = pretty_midi.PrettyMIDI(midi_file)\n",
        "        notes = []\n",
        "        for instrument in midi_data.instruments:\n",
        "            if not instrument.is_drum:\n",
        "                for note in instrument.notes:\n",
        "                    notes.append([note.start, note.end, note.pitch])\n",
        "        return np.array(notes)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {midi_file}: {e}\")\n",
        "        return None\n",
        "\n",
        "def augment_data(notes, shift_range=2):\n",
        "    augmented_data = []\n",
        "    for shift in range(-shift_range, shift_range + 1):\n",
        "        if shift == 0:\n",
        "            continue\n",
        "        shifted_notes = notes.copy()\n",
        "        shifted_notes[:, 2] += shift\n",
        "        augmented_data.append(shifted_notes)\n",
        "    return augmented_data\n",
        "\n",
        "# Load and preprocess MIDI files\n",
        "midi_files, composers = load_midi_files(base_dir, composers)\n",
        "\n",
        "preprocessed_data = []\n",
        "labels = []\n",
        "\n",
        "for midi_file, composer in zip(midi_files, composers):\n",
        "    notes = midi_to_notes(midi_file)\n",
        "    if notes is not None:\n",
        "        augmented_notes = augment_data(notes)\n",
        "        preprocessed_data.extend([notes] + augmented_notes)\n",
        "        labels.extend([composer] * (len(augmented_notes) + 1))\n",
        "\n",
        "# Convert labels to numerical values\n",
        "preprocessed_data = np.concatenate(preprocessed_data)\n",
        "np.save('preprocessed_data.npy', preprocessed_data)\n",
        "np.save('labels.npy', labels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58jdBT3dZeGj"
      },
      "source": [
        "To explain the output: the code handled the errors gracefully by skipping the problematic MIDI files and printing the above and relevant error messages. The warning from pretty_midi about the non-zero tracks is not critical, but it indicates that some MIDI files might not conform to the expected format."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRsmpeZWZeGj"
      },
      "source": [
        "Some Summary Statistics for the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBz7AwWjZeGk",
        "outputId": "f3372b5e-ea23-425f-936c-98221170054a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Summary Statistics for Features:\n",
            "              start      duration         pitch\n",
            "count  2.446408e+07  2.446408e+07  2.446408e+07\n",
            "mean   3.068068e+02  3.209468e-01  6.403889e+01\n",
            "std    3.571839e+02  4.817936e-01  1.230178e+01\n",
            "min    0.000000e+00  4.166667e-04  7.000000e+00\n",
            "25%    8.597463e+01  1.153845e-01  5.600000e+01\n",
            "50%    2.105478e+02  1.898735e-01  6.500000e+01\n",
            "75%    4.153311e+02  3.513514e-01  7.300000e+01\n",
            "max    5.164605e+03  9.863928e+01  1.090000e+02\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Load the extracted features and labels\n",
        "features = np.load('features.npy', allow_pickle=True)\n",
        "labels = np.load('labels.npy')\n",
        "\n",
        "#  Concatenate the nested feature arrays into a single array for easier analysis.\n",
        "flattened_features = np.concatenate(features)\n",
        "\n",
        "# Convert the flattened feature array into a DataFrame for easier manipulation and summary statistics calculation.\n",
        "df_features = pd.DataFrame(flattened_features, columns=['start', 'duration', 'pitch'])\n",
        "\n",
        "#  I'm using 'describe' method to generate summary statistics for the features, including count, mean, standard deviation, min, max, and quartiles.\n",
        "print(\"Summary Statistics for Features:\")\n",
        "print(df_features.describe())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQbrvuzjZeGk"
      },
      "source": [
        "Start Times: The wide range (from 0 to 5164 seconds) and high standard deviation indicate that notes are spread out over a long duration, which is typical for musical pieces with varying lengths.\n",
        "Durations: Most notes have relatively short durations, as indicated by the mean (0.32 seconds) and the quartiles. However, there are some notes with much longer durations (up to 98.64 seconds), which might be sustained notes.\n",
        "Pitches: The pitches are centered around 64, with most notes falling between 56 and 73. This range is typical for classical music, which often centers around the middle of the piano keyboard."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ellComB7ZeGk",
        "outputId": "ca8ee518-138d-45c8-ffe9-6465d7929691"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of samples (features): 24464075\n",
            "Shape of each feature array: (1, 3)\n",
            "Number of samples (labels): 7665\n",
            "Memory usage (features): 559.94 MB\n",
            "Memory usage (labels): 0.26 MB\n",
            "Total memory usage: 560.20 MB\n",
            "Shape of flattened features array: (24464075, 3)\n",
            "\n",
            "Features DataFrame info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 24464075 entries, 0 to 24464074\n",
            "Data columns (total 3 columns):\n",
            " #   Column    Dtype  \n",
            "---  ------    -----  \n",
            " 0   start     float64\n",
            " 1   duration  float64\n",
            " 2   pitch     float64\n",
            "dtypes: float64(3)\n",
            "memory usage: 559.9 MB\n",
            "None\n",
            "\n",
            "Labels DataFrame info:\n",
            "count     7665\n",
            "unique       4\n",
            "top       Bach\n",
            "freq      4650\n",
            "Name: label, dtype: object\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Load the extracted features and labels\n",
        "features = np.load('features.npy', allow_pickle=True)\n",
        "labels = np.load('labels.npy')\n",
        "\n",
        "# Print the number of samples and shape of each feature array\n",
        "print(\"Number of samples (features):\", len(features))\n",
        "print(\"Shape of each feature array:\", features[0].shape if len(features) > 0 else \"N/A\")\n",
        "\n",
        "# Print the number of labels\n",
        "print(\"Number of samples (labels):\", len(labels))\n",
        "\n",
        "# Calculate memory usage for features and labels\n",
        "feature_size = sum([f.nbytes for f in features]) if len(features) > 0 else 0\n",
        "label_size = labels.nbytes\n",
        "total_size = feature_size + label_size\n",
        "\n",
        "# Print memory usage\n",
        "print(\"Memory usage (features): {:.2f} MB\".format(feature_size / (1024 * 1024)))\n",
        "print(\"Memory usage (labels): {:.2f} MB\".format(label_size / (1024 * 1024)))\n",
        "print(\"Total memory usage: {:.2f} MB\".format(total_size / (1024 * 1024)))\n",
        "\n",
        "# Print the shape of the flattened features array for an overview\n",
        "flattened_features = np.concatenate(features)\n",
        "print(\"Shape of flattened features array:\", flattened_features.shape)\n",
        "\n",
        "# Convert the features and labels to a DataFrame for an overview\n",
        "df_features = pd.DataFrame(flattened_features, columns=['start', 'duration', 'pitch'])\n",
        "df_labels = pd.Series(labels, name='label')\n",
        "\n",
        "# Print basic information about the DataFrame\n",
        "print(\"\\nFeatures DataFrame info:\")\n",
        "print(df_features.info(memory_usage='deep'))\n",
        "print(\"\\nLabels DataFrame info:\")\n",
        "print(df_labels.describe())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42DaD0SGZeGk"
      },
      "source": [
        "The dataset consists of 24,464,075 feature samples and 7,665 label samples. Each feature sample has a shape of (1, 3), indicating that each sample is represented by three features: start, duration, and pitch. The total memory usage for the features is approximately 559.94 MB, while the labels occupy around 0.26 MB, resulting in a combined memory usage of 560.20 MB. The flattened features array has a shape of (24,464,075, 3), confirming that there are 24,464,075 individual note records, each with three attributes.\n",
        "\n",
        "The features are stored as floating-point numbers (float64), and the memory usage of the DataFrame containing these features aligns with the calculated memory usage. The labels DataFrame, containing 7,665 entries, indicates that there are four unique composers in the dataset. The most frequent composer in the labels is Bach, with 4,650 entries. This summary provides a clear understanding of the dataset's size, memory footprint, and distribution of the composer labels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3nGu3UTZeGk"
      },
      "source": [
        "**FEATURE EXTRACTION**\n",
        "\n",
        "Now, I'LL proceed with the feature extraction step. I'll extract features such as start time, duration, and pitch from the preprocessed notes. This will help in training the models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "136hRzDhZeGk",
        "outputId": "628f3b99-98f3-4929-ffbf-c42c52c22bad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Feature extraction completed.\n"
          ]
        }
      ],
      "source": [
        "# The extract_features function calculates the start time, duration, and pitch for each note.\n",
        "def extract_features(notes):\n",
        "    features = []\n",
        "    if len(notes.shape) == 1:\n",
        "        start, end, pitch = notes\n",
        "        duration = end - start\n",
        "        features.append([start, duration, pitch])\n",
        "    else:\n",
        "        for note in notes:\n",
        "            start, end, pitch = note\n",
        "            duration = end - start\n",
        "            features.append([start, duration, pitch])\n",
        "    return np.array(features)\n",
        "\n",
        "# Load the preprocessed data and labels\n",
        "preprocessed_data = np.load('preprocessed_data.npy', allow_pickle=True)\n",
        "labels = np.load('labels.npy')\n",
        "\n",
        "# Apply the extract_features function to each set of notes in the preprocessed data.\n",
        "features = [extract_features(notes) for notes in preprocessed_data]\n",
        "\n",
        "# Save the extracted features for further use in model building\n",
        "np.save('features.npy', features)\n",
        "\n",
        "print(\"Feature extraction completed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zuerbm_4ZeGk"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}